\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{textcomp}
\usepackage{hyperref}  % TODO: see page 94 of latex book
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{relsize}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}



\title{CSCI 181 / E-181 Spring 2014 \\ 
{\large 2nd midterm review}
}
\author{
  David Wihl\\
  \texttt{davidwihl@gmail.com}
}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{Support Vector Machines}

\subsection{Background}
Characteristics of SVMs:
\begin{itemize}
	\item \emph{stock} -- SVMs are "off the shelf" and ready to use. No special modification is necessary.
	\item \emph{linearly separable} -- assumes that linear separation is possible. Used natively as a binary classifier.
	\item \emph{convex optimization}. SVM originated as a backlash against neural nets due to nets' non-convexity. In Neural Nets, results were often non-reproducible as different researchers found different results due to different initializations.
	\item \emph{global optimum} -- SVMs will find the global optimum.
\end{itemize}

SVMs are based on three "big ideas":
\begin{itemize}
	\item \emph{margin} Maximizes distance between the closest points
	\item \emph{duality} Take a hard problem and transform it into an easier problem to solve.
	\item \emph{kernel trick} Map input vectors to higher dimensional, more expressive features.
\end{itemize}

\subsection{Definitions}

\begin{description}
	\item[Data:] $\{x_n,t_n\}_{n=1}^N, t_n \in \{-1, +1\}$. $t_n$ is the target or the expected result of the classification.
	\item[J Basis functions:] $\phi_j(x)\rightarrow\Re$, therefore
	\begin{description}
		\item[Vector function:] $\Phi X \rightarrow \Re^J$ produces a column vector.
	\end{description}
	\item[Objective function:]
$f(\textbf{x},\textbf{w},b) = \textbf{w}^\intercal \mathbf{\phi}(\textbf{x})  + b$
where b is the bias.
\end{description}

The sign of $f(\cdot)$ will determine classification $(-1,+1)$

So the actual classifier will be:
\[
y(\textbf{x},\textbf{w},b) =
	\begin{cases}
	  	+1, &\text{if  }\textbf{w}^\intercal \mathbf{\phi}(\textbf{x}) + b > 0 \\
		-1, &\text{otherwise}
	\end{cases}
\]

Unlike Logistic Regression (which uses $\{0, 1\}$), it is preferable to use $\{-1, +1\}$ as the classification result. If $t_n * y$ is positive, then the produced classification is correct (positive $\times$ positive is positive, negative $\times$ negative is also positive).

\emph{Decision Boundary} is the hyperplane where $\textbf{w}^\intercal \phi(\textbf{x}) +b = 0$ . We want to find the Decision Boundary that creates the most separation between the two different classes by maximizing the distance between the two closest points. The distance between the Decision Boundary and the closest point is called the \emph{margin}. The points closest to the Decision Boundary are called the \emph{support vectors}.

\subsection {Max Marginalization}

The margin is determined by the orthogonal distance from the closest point to the Decision Boundary: 
\begin{equation}
\frac{|\textbf{w}^\intercal \textbf{x} + b|}{||\textbf{w}||}
\end{equation}


Maximizing the margin can be written as:
\begin{equation}
\argmax_{w,b} \left\{\min_n(t_n\cdot(\mathbf{w}^\intercal\mathbf{x}+b))\cdot\frac{1}{||w||} \right\}
\end{equation}

Maximizing the margin helps ensure that points which are close to margin will not be pushed over the boundary by noise.

$\mathbf{w}$ is orthogonal to vectors in the Decision Boundary. Here's how: pick two points on the Decision Boundary $\phi(x_1) \text{ and } \phi(x_2)$. So
\begin{align}
\mathbf{w}&^\intercal\left(\phi(x_2) - \phi(x_1)\right) = 0 \text{ for orthogonal dot product}\nonumber \\
\mathbf{w}&^\intercal\phi(x_2) - \mathbf{w}^\intercal\phi(x_1) = 0 \nonumber \\
\text{Note: } \phi&(x_n) = (-b) \nonumber \\
=&(-b) - (-b) \nonumber \\
=& 0 \nonumber
\end{align}


\subsection {Duality}

\subsection {Kernel Tricks}
Mercer function, infinite dimensions (justification for duality)

Slack variables to break the linear separability.

\subsection{Sources}

\begin{enumerate}
	\item Lecture 14, March 24, 2014
	\item Lecture 15
	\item Bishop 6.0-6.2 
	\item Bishop 7.0-7.1
	\item Course notes - maxmargin
	\item Section 7 review
	\item Section 8 review
	\item Machine Learning in Action, Chapter 6
\end{enumerate}

\section{Markov Decision Processes}
Lecture 16

Course notes - MDP

Section 9

\subsection{Partially Observable MDP}
Course notes - POMDP

Section 10

\subsection{Hidden Markov Models}
Bishop 13.0-13.2

\subsection{Mixture Models}
Bishop 9.0-9.2

\section{Reinforcement Learning}
Course notes - RL

Section 9

\subsection{Value and Policy Iteration}
Lecture 17

Course notes - policyiter

\section{Expectation Maximization}

Bishop 9.3

Section 11


\end{document}  
