\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{textcomp}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}  % TODO: see page 94 of latex book
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{framed}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}



\title{CSCI 181 / E-181 Spring 2014 \\ 
{\large 2nd midterm review}
}
\author{
  David Wihl\\
  \texttt{davidwihl@gmail.com}
}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{Support Vector Machines}

\subsection{Background}
Characteristics of SVMs:
\begin{itemize}
	\item \emph{stock} -- SVMs are ``off the shelf" and ready to use. No special modification is necessary.
	\item \emph{linearly separable} -- assumes that linear separation is possible. Used natively as a binary classifier.
	\item \emph{convex optimization}. SVM originated as a backlash against neural nets due to nets' non-convexity. In Neural Nets, results were often non-reproducible as different researchers found different results due to different initializations.
	\item \emph{global optimum} -- SVMs will find the global optimum.
\end{itemize}

SVMs are based on three ``big ideas":
\begin{itemize}
	\item \emph{margin} Maximizes distance between the closest points
	\item \emph{duality} Take a hard problem and transform it into an easier problem to solve.
	\item \emph{kernel functions} map input vectors into higher dimensional, more expressive features to avoid costly computations.
\end{itemize}

\subsection{Definitions}

\begin{description}
	\item[Data:] $\{x_n,t_n\}_{n=1}^N, t_n \in \{-1, +1\}$. $t_n$ is the target or the expected result of the classification.
	\item[J Basis functions:] $\phi_j(x)\rightarrow\mathbb{R}$, therefore
	\begin{description}
		\item[Vector function:] $\Phi X \rightarrow \mathbb{R}^J$ produces a column vector.
	\end{description}
	\item[Objective function:]
$f(\textbf{x},\textbf{w},b) = \textbf{w}^\intercal \mathbf{\phi}(\textbf{x})  + b$
where b is the bias.
\end{description}

The sign of $f(\cdot)$ will determine classification $(-1,+1)$

So the actual classifier will be:
\[
y(\textbf{x},\textbf{w},b) =
	\begin{cases}
	  	+1, &\text{if  }\textbf{w}^\intercal \mathbf{\phi}(\textbf{x}) + b > 0 \\
		-1, &\text{otherwise}
	\end{cases}
\]

Unlike Logistic Regression (which uses $\{0, 1\}$), it is preferable to use $\{-1, +1\}$ as the classification result. If $t_n * y$ is positive, then the produced classification is correct (positive $\times$ positive is positive, negative $\times$ negative is also positive).

\emph{Decision Boundary} is the hyperplane where $\textbf{w}^\intercal \phi(\textbf{x}) +b = 0$ . We want to find the Decision Boundary that creates the most separation between the two different classes by maximizing the distance between the two closest points. The distance between the Decision Boundary and the closest point is called the \emph{margin}. The points closest to the Decision Boundary are called the \emph{support vectors}.

\subsection {Max Marginalization}

The margin is determined by the orthogonal distance from the closest point to the Decision Boundary: 
\begin{equation}
\frac{|\textbf{w}^\intercal \textbf{x} + b|}{||\textbf{w}||}
\end{equation}


Maximizing the margin can be written as:
\begin{equation}
\argmax_{w,b} \left\{\min_n(t_n\cdot(\mathbf{w}^\intercal\mathbf{x}+b))\cdot\frac{1}{||w||} \right\}
\end{equation}

Maximizing the margin helps ensure that points which are close to margin will not be pushed over the boundary by noise.

$\mathbf{w}$ is orthogonal to vectors in the Decision Boundary. Here's how: pick two points on the Decision Boundary $\phi(x_1) \text{ and } \phi(x_2)$. So
\begin{align}
\mathbf{w}&^\intercal\left(\phi(x_2) - \phi(x_1)\right) = 0 \text{ for orthogonal dot product}\nonumber \\
\mathbf{w}&^\intercal\phi(x_2) - \mathbf{w}^\intercal\phi(x_1) = 0 \nonumber \\
\text{Note: } \mathbf{w}&^\intercal\phi(x_n) = (-b) \text{, so} \nonumber \\
=&(-b) - (-b) \nonumber \\
=& 0 \nonumber
\end{align}

Since $\mathbf{w}$ is orthogonal, we want to maximize it. It is not unit length, but could be, by scaling with a factor of $r$.

The margin is defined where
\begin{equation}
\mathbf{w}^\intercal\phi(x)+b = \pm 1
\end{equation}

The origin can be found at $\frac{b}{||w||}$.

See \href{http://cs229.stanford.edu/notes/cs229-notes3.pdf}{Stanford CS229 SVM Notes} re: Functional vs. Geometric Margins

The support vector is defined as $r\frac{\mathbf{w}}{||\mathbf{w}||_2}$, where $r$ is multiplied by the unit vector orthogonal to the Decision Boundary hyperplane. 

Define the point where the vector meets the plane as $\phi_\perp (\mathbf{x})$, so
\begin{equation}
\phi(\mathbf{x}) = \phi_\perp(\mathbf{x}) + r\frac{\mathbf{w}}{||\mathbf{w}||_2}
\end{equation}

Solving for $r$, multiple both sides by $\mathbf{w}^\intercal$.

(Recall: $\mathbf{w}^\intercal\mathbf{w} = ||\mathbf{w}||^2$)

\begin{align}
\mathbf{w}^\intercal\phi(\mathbf{x}) = &\mathbf{w}^\intercal  \phi_\perp(\mathbf{x})  +	 r\frac{\mathbf{w}^\intercal\mathbf{w}}{||\mathbf{w}||} \\
			 = & (-b) + r||\mathbf{w}||
\end{align}
Therefore, the margin for a point $\mathbf{x}$.
\begin{align}
r = &\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||w||} \\
  = & \frac{f(\mathbf{x},\mathbf{w},b)}{||w||}
\end{align}
This makes it easy to calculate how far away a point is from the Decision Boundary. $r$ is strictly not a length because it could be negative. However, we only care about the actual distance to the boundary.

Margin for a datum $n$:
\begin{equation}
margin = t_n\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||\mathbf{w}||}
\end{equation}

This is getting close to a loss function as we can now figure out the worst of these. The Margin for all the training data will be the point closest to the Decision Boundary:
\begin{equation}
min_n\left\{ t_n\frac{\phi(\mathbf{x})^\intercal\mathbf{w}+b}{||\mathbf{w}||} \right\}
\end{equation}

As mentioned at the beginning of this section, the Objective Function is 
\begin{equation}
\mathbf{w}^*,b^*=
\argmax_{w,b} \left\{\min_n(t_n\cdot(\phi(\mathbf{x})^\intercal\mathbf{w}+b))\cdot\frac{1}{||w||} \right\}
\end{equation}

but now we can simplify some things. $\mathbf{w}$ are $b$ are scale free (if we multiply by some $\beta$, the max and min will still be the same.)

Let's define a set of linear constraints such that the margin is always $\ge 1$ to make this easier to solve.
\begin{align}
\mathbf{w}^*,b^*= \argmax_{\mathbf{w},b}\frac{1}{||w||}
\end{align}
subject to 
\begin{align}
t_n\cdot(\phi(x_n)^\intercal\mathbf{w}+b) \ge 1 \:\forall\: n
\end{align}
This can be made even easier. Finding the max of $\frac{1}{||w||}$ is like finding the min of $||w||^2$, so
\begin{align}
\mathbf{w}^*,b^*=& \argmin_{\mathbf{w},b}||w||^2 \\
\text{s.t.} \:& t_n(\phi(x_n)^\intercal\mathbf{w} + b) \ge 1
\end{align}
so this reduces to just a quadratic program (QP) with linear constraints that could be solved by any number of commercial packages and produces a global minimum.

\subsection{Slack Variables}
If the data is not strictly linearly separable, it can mitigated by slack variables.

$\xi_n \leftarrow$ one for each datum. 
\begin{description}
 \item[$\xi_n = 0$] then the datum is correctly classified and outside the margin.
 \item[$0 < \xi_n <= 1$] the datum is correctly classified and within the margin
 \item[$\xi_n > 1$] the datum is misclassified
\end{description}

We will now add $\xi$ as a constraint to minimize.
\begin{equation}
t_n\cdot(\phi(x_n)^\intercal\mathbf{w}+b) \ge 1 - \xi_n
\end{equation}
New objective function:
\begin{align}
\mathbf{w}^*,b^*,\xi^*=& \argmin_{\mathbf{w},b,\mathbf{\xi}}\left\{||w||^2+ c \sum_{n=1}^{N}\xi_n\right\}\\
\text{s.t.} \:& t_n(\phi(x_n)^\intercal\mathbf{w} + b) \ge 1 -\xi_n\\
& \xi \ge 0\\
\forall \:n
\end{align}
where $c>0$ is the regularization parameter. A small $C$ means that you don't care much about errors. The sum of $\xi$ is the upper bound on how many can be wrong. If $c = 0$, it becomes the original function.
Typically,
\begin{equation}
c=\frac{1}{\nu N} \text{, where } 0 < \nu \le 1
\end{equation}
where $\nu$ is the tolerance for percentage willing to get wrong.

\subsection {Duality}
\begin{framed}
First a recap of Lagrange Multipliers.

Lagrange multipliers solve for $f(x,y,z)$ subject to $g(x,y,z) = k$ where $g(\cdot)$ is the \emph{constraint}.

Form:
\begin{equation}
F(x,y,z,\lambda) = f(x,y,z) - \lambda(g(x,y,z)-k)
\end{equation}
then solve for
$F_x = 0, F_y=0, F_z = 0, F_\lambda = 0$ using partial derivatives which will provide a max or min.
\end{framed}

Maximizing the margin:
\begin{align}
\mathbf{w}^*,b^*= &
\argmax_{w,b} \left\{\min_n(t_n\cdot(\mathbf{w}^\intercal\phi(\mathbf{x})+b))\cdot\frac{1}{||w||} \right\}\\
= &\argmax_{w,b} \frac{1}{||w||}\min_n(t_n\cdot(\mathbf{w}^\intercal\phi(\mathbf{x})+b))
\end{align}
This is still hard to differentiate. This is scalable by $\beta$.

We want to try a lot of basis functions. Duality allows us to try weights on \emph{data}, rather than basis functions. This allows an infinite number of dimensions. 

Solution: use duality. Associate a scalar with each constraint $\mathbf{\alpha} = [\alpha_1,\alpha_2,...,\alpha_N]$, each of which must be non-negative. Each constraint has it's own $\alpha$  which serves as the Lagrange multiplier for each constraint.
\begin{equation}
\mathcal{L}(\mathbf{w},b,\mathbf{\alpha}) = \frac{1}{2}||w||^2-\sum_{n=1}^N \alpha_n(t_n(\mathbf{w}^\intercal\phi(\mathbf{x})+b ) -1)
\end{equation}
The summation term will be negative if the constraint is violated. By throwing a minus sign in front and then maximizing $\alpha$ will make the term really large (maybe $\infty$). This will make $\min()$ unhappy - causing a huge value in the loss function. It is a math trick to show a constraint has been violated.
\begin{equation}
\mathbf{w}^*,b^*= \argmin_{\mathbf{w},b} \max_{\alpha \ge 0} \mathcal{L}(\mathbf{w},b,\mathbf{\alpha})
\end{equation}
The problem is now restated as a function of $\alpha$, not a $\mathbf{w},b$ problem. This is duality.

\emph{Weak duality}: the solution to an $\alpha$ problem makes the lower bounds of the w,b problem. Max $\alpha$ = min w,b. See formula 20 in maxmargin notes.

Strong duality: min and max can be switched without changing the answer. See formula 21 in maxmargin notes.
\begin{equation}
\frac{\partial L}{\partial \mathbf{w}} = 0 
\end{equation}

\begin{equation}
\frac{\partial L}{\partial b} = 0 
\end{equation}

$\alpha$'s are sparse so a lot of these terms will disappear. The remaining terms will provide the values as support vectors. Rewriting the Lagrangian just in terms of alpha can be found in formula 28 in maxmargin notes.

See formulas 28, 29 in maxmargin notes. This is a relatively easier problem to solve as it involves just the alphas.

Either alpha = 0, or $t_n(w^*\phi(x_n)+b^*) = 0$ These latter terms will be closest to the margin (they will be ``tightest"). A small subset of the data determine the boundary. Train on all of data and throw away most of the data in order to have the classifier.

The only time we need to compute something J dimensional is on the inner product. See $\phi$ term.

\subsection {Kernel Tricks}


``A kernel function is a scalar product on two vectors mapped by basis functions into a feature space. In general, we use kernels to map into higher dimensional feature spaces, using them to circumvent costly computations in high dimension spaces."

\begin{equation}
K(\mathbf{x},\mathbf{x}') = \phi^\intercal(\mathbf{x})\phi(\mathbf{x}')
\end{equation}

Kernel functions can be generalized to any distance measure. The larger K means the distance is closer. This can be applied to text, proteins, etc. (see Alpaydin example).

Exponentiate the negative squared distance of two dissimilar things, e.g.
\begin{equation}
K(x,z) = exp\left\{ -\frac{1}{2} ||x - z||^2 \right\}
\end{equation}

Don't engineer features - engineer distances. Feature space can be infinite (another way of saying this is that distances can be continuous as in a Gaussian distribution). Don't worry about features - worry about distance between things. Then create a kernel and use distance to discriminate.

Bayesian linear regression where all features interact as inner product so it can be turned into a kernel function. This can be a Gaussian kernel. Non-parametric infinite dimensional models using finite computers. Can also be used with PCA.

Mercer function, infinite dimensions (justification for duality). See also QR decomposition for large datasets.

Be cautious of pathological distance functions (as opposed to feature functions).

CS229 goes into a lot more depth about this: Gaussian kernels in depth, plus SMO and Karush-Kuhn-Tucker (KKT) conditions.


\subsection{Sources}

\begin{enumerate}
	\item Lecture 14, March 24, 2014
	\item Lecture 15, March 31, 2014
	\item Bishop 6.0-6.2 
	\item Bishop 7.0-7.1
	\item Course notes - maxmargin
	\item Section 7 review
	\item Section 8 review
	\item \href{http://cs229.stanford.edu/notes/cs229-notes3.pdf}{Stanford CS229 SVM notes}
	\item Machine Learning in Action, Chapter 6
\end{enumerate}

\section{Reinforcement Learning}

Online, related to planning. 

\subsection{Markov Decision Processes}

States: $\mathcal{S} = \{1,2,3,...N\}$

Actions: $\mathcal{A} = \{1,2,3,...M\}$ which move us from state to state over a probability distribution. (You don't always land where you expected to go.). This noisy movement is defined by:

Transition Model $p(s'\mid s,a) \leftarrow$  PMF, indexed by state, action

Reward Function: $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$

Policy: $\pi_t: \mathcal{S} \rightarrow \mathcal{A}$. Finding the right policy is the objective here.

Assumptions (for now):\\
These assumptions will be relaxed later.
\begin{enumerate}
	\item \emph{fully observable} You know what state you are in.
	\item \emph{known model}
	\item \emph{Markov Property} meaning the way the world works in the future is completely summarized by the current state. The past is not relevant. ``The future is independent of the past given the present."
	\item \emph{Finite actions and states}
	\item \emph{Bounded Rewards} - there is a maximum reward that dominates all rewards over all states.
\end{enumerate}

Agent could live forever (so it tries to maximize average reward) or lives a finite time (so it tries to maximize short term reward). Trade off of exploration vs. exploitation

Different types of utility ($u$) function:
\begin{description} \itemsep5pt
	\item[Finite Horizon*]	$u = \sum_{t=0}^{T-1}\mathcal{R}(s_t, a_t)$
	\item[Total Reward]$u = \sum_{t=0}^\infty\mathcal{R}(s_t,a_t)$ (may not converge)
	\item[Total Discounted Reward*]$u = \sum_{t=0}^\infty\gamma^t\mathcal{R}(s_t,a_t)$ where the discount factor $\gamma = [0,1)$
	\item[Long-run Average Reward]$u= \lim_{T\rightarrow\infty}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{R}(s_t,a_t)$
\end{description}

* Most common and tractable utility functions

Value means ``make a good move" in the anticipation of getting a reward.

\subsection{Expectimax}

Take an action to maximize utility. Nature responds randomly. For now, we know everything about everything except what nature will do.


\subsection{Value Iteration}

\subsection{Policy Iteration}

\subsection{Reinforcement Learning}

\subsection{Partially Observable MDP}

\subsection{Mixture Models}

\subsection{Sources}

\begin{enumerate}
	\item Lecture 16, April 2, 2014
	\item Lecture 17
	\item Lecture 18
	\item Course notes - MDP
	\item Course notes - policyiter
	\item Course notes - RL
	\item Course notes - POMDP
	\item Bishop 9.0-9.2
	\item Section 9 review
	\item Section 10 review
	\item \href{http://cs229.stanford.edu/notes/cs229-notes12.pdf}{CS229 Reinforcement Learning and Control}
\end{enumerate}

\section{Expectation Maximization}

\subsection{Hidden Markov Models}

\subsection{Sources}

\begin{enumerate}
	\item Bishop 9.3
	\item Bishop 13.0-13.2
	\item Section 11
	\item \href{http://cs229.stanford.edu/notes/cs229-notes7b.pdf}{CS229 Mixture of Gaussians}
	\item \href{http://cs229.stanford.edu/notes/cs229-notes8.pdf}{CS229 EM Notes}
\end{enumerate}

\end{document}  
