\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{textcomp}
\usepackage{hyperref}  % TODO: see page 94 of latex book
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{parskip}

\title{CSCI 181 / E-181 Spring 2014 \\ 
{\large 2nd midterm review}
}
\author{
  David Wihl\\
  \texttt{davidwihl@gmail.com}
}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{Support Vector Machines}

\subsection{Max-Margin Classification}
SVMs are based on three "big ideas":
\begin{itemize}
	\item \emph{margin}. Maximizes distance between the closest points
	\item \emph{duality}. Take a hard problem and transform it into an easier problem to solve.
	\item \emph{kernel trick}. Map input vectors to higher dimensional, more expressive features.
\end{itemize}

Characteristics of SVMs:
\begin{itemize}
	\item \emph{linearly separable}. assumes that linear separation is possible
	\item \emph{convex optimization}. SVM originated as a backlash against neural nets due to non-convexity. In Neural Nets, results were often non-reproducible as different researchers found different results.
	\item \emph{global optimum} SVMs will find the global optimum.
\end{itemize}


DATA: $\{x_n,t_n\}_{n=1}^N, t_n \in \{-1, +1\}$

J Basis functions: $\phi_j(x)\rightarrow\Re$

Vector function: $\Phi X \rightarrow \Re^J$ (column vector)

Assume linear separability

Objective function: $f(\vec{x},\vec{w},b) = \phi(\vec{x})^\intercal \vec{w} + b$
where b is the bias or offset.
The sign of $f(\dot)$ will determine classification $(-1,+1)$

SVM is used as a classifier, such that:
\begin{eqnarray*}
y(\vec{x},\vec{w},b) &=&
  \left\{\begin{array}{ll}
  	+1, &\mbox{if  }  \vec{\Phi}(\vec{x})^\intercal \vec{w} + b > 0 \\
	-1, &\mbox{otherwise}
  \end{array}\right. \\
\end{eqnarray*}

Decision boundary is the hyperplane where $\vec{\phi}(\vec{x})^\intercal\vec{w}+b = 0$

Sources:
\begin{enumerate}
	\item Lecture 14
	\item Bishop 6.0-6.2 
	\item Course notes - maxmargin
	\item Section 7 review
\end{enumerate}


\subsection{SVM}
Lecture 15

Bishop 7.0-7.1

Section 8

\section{Markov Decision Processes}
Lecture 16

Course notes - MDP

Section 9

\subsection{Partially Observable MDP}
Course notes - POMDP

Section 10

\subsection{Hidden Markov Models}
Bishop 13.0-13.2

\subsection{Mixture Models}
Bishop 9.0-9.2

\section{Reinforcement Learning}
Course notes - RL

Section 9

\subsection{Value and Policy Iteration}
Lecture 17

Course notes - policyiter

\section{Expectation Maximization}

Bishop 9.3

Section 11


\end{document}  
