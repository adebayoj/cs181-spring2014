\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{textcomp}
\usepackage{hyperref}  % TODO: see page 94 of latex book
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{parskip}

\title{CSCI 181 / E-181 Spring 2014 \\ 
{\large 2nd midterm review}
}
\author{
  David Wihl\\
  \texttt{davidwihl@gmail.com}
}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{Support Vector Machines}

\subsection{Background}
Characteristics of SVMs:
\begin{itemize}
	\item \emph{stock} -- SVMs are "off the shelf" and ready to use. No special modification is necessary.
	\item \emph{linearly separable} -- assumes that linear separation is possible. Used natively as a binary classifier.
	\item \emph{convex optimization}. SVM originated as a backlash against neural nets due to nets' non-convexity. In Neural Nets, results were often non-reproducible as different researchers found different results due to different initializations.
	\item \emph{global optimum} -- SVMs will find the global optimum.
\end{itemize}

SVMs are based on three "big ideas":
\begin{itemize}
	\item \emph{margin} Maximizes distance between the closest points
	\item \emph{duality} Take a hard problem and transform it into an easier problem to solve.
	\item \emph{kernel trick} Map input vectors to higher dimensional, more expressive features.
\end{itemize}

\subsection{Setup}

\begin{description}
	\item[Data:] $\{x_n,t_n\}_{n=1}^N, t_n \in \{-1, +1\}$
	\item[J Basis functions:] $\phi_j(x)\rightarrow\Re$, therefore
	\begin{description}
		\item[Vector function:] $\Phi X \rightarrow \Re^J$ produces a column vector.
	\end{description}
	\item[Objective function:]
$f(\textbf{x},\textbf{w},b) = \textbf{w}^\intercal \mathbf{\phi}(\textbf{x})  + b$
where b is the bias.
\end{description}

The sign of $f(\cdot)$ will determine classification $(-1,+1)$

So the actual classifier will be:
\begin{eqnarray*}
y(\textbf{x},\textbf{w},b) &=&
  \left\{\begin{array}{ll}
  	+1, &\mbox{if  }  \textbf{w}^\intercal \mathbf{\phi}(\textbf{x}) + b > 0 \\
	-1, &\mbox{otherwise}
  \end{array}\right. \\
\end{eqnarray*}

Decision boundary is the hyperplane where $\textbf{w}^\intercal \phi(\textbf{x}) +b = 0$ . We want to find the decision boundary that creates the most separation between the two different classes by maximizing the distance between the two closest points. The points closest to the separating hyperplane are known as the \emph{support vectors}.

The distance from the decision boundary to the closest point is determined by: 
\begin{equation}
\frac{|\textbf{w}^\intercal \textbf{x} + b|}{||\textbf{w}||}
\end{equation}

\subsection {Duality}

\subsection {other stuff}
Mercer function, infinite dimensions (justification for duality)

\subsection{Sources}

\begin{enumerate}
	\item Lecture 14, March 24, 2014
	\item Lecture 15
	\item Bishop 6.0-6.2 
	\item Bishop 7.0-7.1
	\item Course notes - maxmargin
	\item Section 7 review
	\item Section 8 review
	\item Machine Learning in Action, Chapter 6
\end{enumerate}

\section{Markov Decision Processes}
Lecture 16

Course notes - MDP

Section 9

\subsection{Partially Observable MDP}
Course notes - POMDP

Section 10

\subsection{Hidden Markov Models}
Bishop 13.0-13.2

\subsection{Mixture Models}
Bishop 9.0-9.2

\section{Reinforcement Learning}
Course notes - RL

Section 9

\subsection{Value and Policy Iteration}
Lecture 17

Course notes - policyiter

\section{Expectation Maximization}

Bishop 9.3

Section 11


\end{document}  
